---
title: |
       | On the Computation of Large Spatial Datasets With the&nbsp;M&nbsp;function
author:
  - name: "Eric Marcon"
    authsuperscript: 1*
  - name: "Florence Puech"
    authsuperscript: 2 
affiliation:
  - affsuperscript: 1
    dptuniv: "AgroParisTech, UMR AMAP, CIRAD, CNRS, INRAE, IRD, Univ Montpellier, Montpellier, France."
  - affsuperscript: 2
    dptuniv: "Université Paris-Saclay, INRAE, AgroParisTech, Paris-Saclay Applied Economics, F-91120 Palaiseau, France."
corrauthor:
  email: eric.marcon@agroparistech.fr
abstract: >
  Increasing access to large individual geolocalised datasets, coupled with the development of computing power, has encouraged the search for suitable spatial statistic tools.
  In this line, distance-based methods have been largely developed in different scientific fields to detect spatial concentration, dispersion or independence of entities at any distance and without any bias. 
  In a recent article, @Tidu2023 highlight the qualities of the *M* function [@Marcon2010], a relative distance-based measure.
  @Tidu2023 also express reservations about *M* for the computation times required.
  In our article, we propose a methodological work that seeks to specify the processing of large spatialized datasets with the *M* function by using R software.
  Two avenues are being explored to determine the computational performance of *M*.
  A precise evaluation of the computational time and memory requirements for geolocalised data is at first carried out using the *dbmss* package in R [@Marcon2014] by means of performance tests. 
  Then, as suggested by @Tidu2023, approximating the geographical positions of the entities analysed is considered.
  The extent of the deterioration in the estimate of *M* induced by this approach is estimated and discussed, as the gains in computation time made possible by the spatial approximation of locations.
  We give evidence that the individual location approximation generates information loss at small distances, implying a tradeoff between the smallest distance at which spatial interactions can be detected and computing performance.
  We recommend designing the analysis of big dasets taking it into account.
  The R code is given for the reproducibility of our results.
keywords: [Distance-based method, M-function, Performance test, R Package dbmss]
journalinfo: "Preprint"
date: "`r format(Sys.time(), '%Y %b %d')`"
archive: "`r format(Sys.time(), '%Y %b %d')`"
keywordlabel: Keywords
corrauthorlabel: Corresponding author
url: https://EricMarcon.github.io/MLargeDataSets/
github-repo: EricMarcon/MLargeDataSets
lang: en-GB
bibliography: references.bib
biblio-style: chicago
csl: apa
toc-depth: 3
fontsize: 10pt
urlcolor: blue
always_allow_html: yes
csquotes: true
output:
  rmdformats::downcute:
    use_bookdown: yes
    lightbox: yes
    toc_depth: 3
    self_contained: no
  bookdown::word_document2:
     reference_docx: "images/template.docx"
     number_sections: false
  bookdown::pdf_book:
    template: latex/stylish_article.tex
    citation_package: natbib
    latex_engine: xelatex
    keep_tex: no
---

```{r DoNotModify, include=FALSE}
### Utilities. Do not modify.
# Installation of packages if necessary
InstallPackages <- function(Packages) {
  InstallPackage <- function(Package) {
    if (!Package %in% installed.packages()[, 1]) {
      install.packages(Package, repos = "https://cran.rstudio.com/")
    }
  }
  invisible(sapply(Packages, FUN = InstallPackage))
}

# Basic packages
InstallPackages(c("bookdown", "formatR", "ragg", "magick", "kableExtra"))

# Chunk font size hook: allows size='small' or any valid Latex font size in chunk options
def.chunk.hook  <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
  x <- def.chunk.hook(x, options)
  ifelse(options$size != "normalsize", paste0("\n \\", options$size,"\n\n", x, "\n\n \\normalsize"), x)
})
```

```{r Options, include=FALSE}
### Customized options for this document
# Add necessary packages here
Packages <- c("tidyverse", "spatstat", "dbmss", "pbapply", "plyr", "GoFKernel", "microbenchmark", "profmem", "gridExtra")
# Install them
InstallPackages(Packages)

# knitr options
knitr::opts_chunk$set(
  cache =   TRUE,     # Cache chunk results
  include = FALSE,    # Show/Hide chunks
  echo =    FALSE,    # Show/Hide code
  warning = FALSE,    # Show/Hide warnings
  message = FALSE,    # Show/Hide messages
  # Figure alignment and size
  fig.align = 'center', out.width = '100%',
  # Graphic devices (ragg_png is better than standard png)
  dev = c("ragg_png", "pdf"),
  # Code chunk format
  tidy = FALSE, tidy.opts = list(blank = FALSE, width.cutoff = 50),
  size = "scriptsize", knitr.graphics.auto_pdf = TRUE
  )
options(width = 50)

# ggplot style
library("tidyverse")
theme_set(theme_bw())
theme_update(panel.background = element_rect(fill = "transparent", colour = NA),
             plot.background = element_rect(fill = "transparent", colour = NA),
             legend.text = element_text(size = 14),
             axis.text.x = element_text(size = 13),
             axis.text.y = element_text(size = 13))
knitr::opts_chunk$set(dev.args = list(bg = "transparent"))

# Random seed
set.seed(973)
```

# Introduction {.unnumbered}

Increasing access to large individual and spatial datasets and greater computing power have encouraged the development of statistical analysis tools for processing such data in the best possible way [@Baddeley2016].
Empirical studies at very fine geographical levels have thus been proposed in recent years for large datasets.
Particular attention has been paid to detect the spatial structures (attraction, repulsion, independence) of individual spatialised data using analyses that are no longer based on zoned data but on geolocalised data.
This type of approach has the advantage of preserving the exact positions of the entities analysed.
It has been proven that any distance-based method (by considering space as continuous) circumvents statistical bias associated with to the Modifiable Areal Unit Problem – MAUP [@Arbia1989; @Openshaw1979] due to discretising space into separate units.
Various studies have shown how important it is to use this type of methodology in many fields, including geography [@Sweeney1998; @Deurloo2008; @Kukuliac2017], economics [@Arbia1989; @Marcon2003a], ecology [@Cressie1993; @Lentz2011], biology [@Dray2021], and so on.

In a recent article, @Tidu2023 highlight the interest of a particular statistical measure, the *M* function proposed by @Marcon2010.
This measure, which we will refer to as *M* in the remainder of the article, makes it possible to highlight spatial structures within a spatialised distribution (attraction, repulsion, independence) from a study based on the distances separating the entities analysed.
However, while this measure preserves all the richness of individual geolocated data, it requires a longer calculation time than other distance-based measures, since it is both a cumulative and relative measure <!--# Eric: je ne suis pas sûr que 'cumulatif' soit pertinent. Le temps de calcul plus long est du à la prise en compte de deux distributions -> 'relatif', mais pas au cumul --> [see @Marcon2012c for a literature review on the advantages and limitations of a dozen existing distance-based measures].
@Tidu2023 propose to limit *M* calculation times by introducing a voluntary positioning error for the entities analysed.
For example, in their study, industrial establishments in Sardinia (Italy) are no longer located at their exact postal address but at the centroid of their municipality.
This repositioning reduces calculation times, as the number of possible distances between establishments is in fact limited to the distances separating the centroids of the municipalities.
This approach is similar to that of @Scholl2013 who proposed, for the $K_d$ function [@Duranton2005] which characterises spatial structures using another method, to approximate the distances between pairs of entities by grouping them into classes.
The method of @Scholl2013, implemented in the *dbmss* package [@Marcon2014] for R [@R] provides a considerable gain in computational performance with little loss of accuracy.
On the other hand, the information loss due to the approximation of the location of objects should imply a loss of accuracy in the estimation of their interactions at the same scale, that must be assessed.

In our paper, we propose to test the effectiveness of @Tidu2023's method and help the researchers to choose the appropriate method to characterise the spatial structure of quite large datasets.
First, we show the advantages of using the *dbmss* package to estimate the *M* function on datasets with an order of magnitude of 100,000 points or less, and we show that the computation times become excessive beyond that, on a personal computer.
We then study the effect of the geographical approximation of the locations of the entities analysed.
This methodological work, based on a deliberately limited number of entity locations, enables us to quantify the extent of the deterioration in information that this approach creates.
These performance tests provide a precise answer to the computational advantages and limitations of the *M* function as a function of the size of the datasets.

The layout of the article is as follows.
The two first sections present the *M* function and the necessary data generated for the tests.
Large point sets (of the order of several tens of thousands of points) that are either completely random or geographically concentrated are drawn.
The third section details the use of the *dbmss* package to calculate the *M* function and its confidence interval from a table giving the position and characteristics of the points or a matrix of distances between them.
The fourth section measures the performance of *dbmss* as a function of the size of the set of points, in terms of computing time and memory requirements.
The fifth section tests the approximation which consists of grouping them together at the centre of the cells of a grid, following the approach of @Tidu2023 which positions them at the centre of the administrative units in which they are located.
In the last section, we conclude and discuss the advantages and the limits of an approximation of the locations on the results as well as on the computing time.

# The *M* function

## Main idea

@Marcon2010 introduced the *M* function that evaluates the dependence between geolocalized points without relying on a specific zoning of space.
As any distance-based methods, the calculation of *M* is based on distances that separate entities under study (establishment, shops...).
The idea of the *M* function is simple: it compares two proportions of neighbours of interest, a local one to a global one.
The local one is defined as the proportion of neighbours of interest within a distance *r*.
The global one is the same proportion but defines on the whole territory.
This comparison of ratios allows the detection of:

-   spatial concentration (attraction) of entities if the proportion of local neighbours is greater than the one observed on the entire territory,
-   spatial dispersion (repulsion) of entities if the relative proportion of local neighbours is lesser than the one observed all over the territory,
-   independence between entities if the local distribution of neighbours does not differ from the global one.

This comparison of proportions of neighbours defines *M* as a *relative* distance-based measure in a strict sense [@Marcon2012c].
The term *topographic* distance-based measures is preferred for those which used the surface area as a benchmark, as the well-known Ripley's *K* function [@Ripley1976; @Ripley1977].
Lastly, the *M* function is a *cumulative* distance-based method because the local environment is defined within a distance *r* and not at a distance *r*.

The possibility to detect exactly at which distance(s) the spatial concentration or dispersion appears coupled with the interpretation of the results open the way to describe very precisely the distribution of entities under study.
Moreover, an easy-computation of *M* is possible thanks to the *dbmss* R package [@Marcon2014].
The *M* function was at first introduced in the field of economics and @Marcon2010 proved that *M* satisfies all of the requirements of @Duranton2005 for the evaluation of spatial distribution of industries.
Since its introduction, various studies have described the spatial locations of industries by using *M* ; for example, @Jensen2011 study the location of shops at a urban level, @CollMartinez2019 analyse the one of the creative industries at a metropolitan level etc.
This methodology has also rapidly been applied in other sciences including biology [@Fernandez2005], geography [@Deurloo2008], ecology [@Marcon2012] or seismology [@Nissi2013].
The *M* function is now included in general textbooks of spatial statistics as in @Arbia2021, but it is less popular than $K_d$'s function of @Duranton2005 [see @Chain2019].

## Definition

The $M$ function is based on the point process theory [@Moller2004; @Baddeley2016].
This distance-based method has been developed to analyse interactions among entities in a context of heteregeneous space.
It means that within this statistical framework, we consider that any entity analysed has not the same probability to locate everywhere on the territory (*first-order intensity property of the point pattern*).
Then after controlling for space heterogeneity, we are able to identify interactions and thus detect spatial concentration or dispersion (*second-order intensity property of the point pattern*).
Controlling for space heterogeneity is a consistent assumption for studying agglomeration of industries (see discussion of @Duranton2005 on that subject).

The definition of the *M* function it as follows.
It compares the relative proportion of entities of interest up to each distance $r$ to the same ratio but defined over the entire territory under study.
In the present article, we only consider the intra-type version of *M*: we study the spatial structure of neighbouring points of the same type (called *points of interest*) as the point at the centre of the disks of radius $r$.
In a mathematical terms, let us denote:

-   $x^s_i$, the geolocalised position of point $i$ of the reference type $s$, at the centre of the disk (the point at which the neighbourhood is to be analyzed),
-   $x^s_j$, the geolocalised position of a neighbour of interest $j$ of the same type as point $i$,
-   $x_j$, the geolocalised of a neighbour $j$ of $i$, whatever its type,
-   $w(.)$, the weight of a given neighbour. In that sense, $w(x_j)$ defines the weight of a neighbor $j$ of $i$.
-   $W_s$, the total weight of the points $x^s_j$,
-   $W$, the total weight of all points of the dataset, whatever their type,
-   ${\mathbf 1} \left( \left\| x^s_i - x_j \right\| \le r \right)$, the indicator function equal to 1 if $x_j$ is in the neighbourhood of $x^s_i$, e.g., the distance between $x^s_i$ and $x_j$ is at most equal to $r$, 0 otherwise.
-   ${\mathbf 1} \left( \left\| x^s_i - x^s_j \right\| \le r \right)$, the indicator function equal to 1 if the distance between $x^s_i$ and $x^s_j$ is at most equal to $r$, 0 otherwise.

The intra-type $M$ function is defined as:

$$
  \hat{M}\left(r\right) = {\sum_i{\frac{\sum_{j \ne i}{{\mathbf 1} \left( \left\| x^s_i - x^s_j \right\| \le r \right) w \left( x^s_j \right)}}{\sum_{j \ne i}{{\mathbf 1} \left( \left\| x^s_i - x_j \right\| \le r \right) w \left( x_j \right)}}}} / {\sum_i{\frac{W_s - w \left( x^s_i \right)}{W - w \left( x^s_i \right)}}}
  (\#eq:M)
$$

A certain number of remarks has to be done.
The first one is that the benchmark value of *M* is equal to 1, whatever the distance considered.
It means that for any given radius *r*:

-   if the estimated $M$ result is above 1, the local value of the ratio is greater than the global one:,a spatial concentration of entities of type $s$ within that radius is thus detected.
-   if the estimated $M$ result is under 1, the local value of the ratio is lesser than the global one: a spatial dispersion of entities of type $s$ within that radius is thus detected.

The second remark concerns the significance of the results.
A confidence interval can be generated thanks to Monte Carlo simulations following @Marcon2010.
A risk level has to be chosen (for example 5%) as well as the number of simulations (the greater the number of simulations, the longer is the duration of the calculation of *M*).
Third, the package *dbmss* [@Marcon2014] on the R software [@R] can be used to compute the $M$ function.
The Euclidean distance is generally preferred for the calculation of *M* but the *dbmss* package also proposed to used network-distances.

# Data simulation

The datasets we will consider in this article are obtained by simulation.
The R code is given in the appendix, which allows perfect reproducibility of the examples treated or the development of others.

## Drawing the points

```{r}
#| label: ParamsCSRCode
library("tidyverse")
library("spatstat")
library("dbmss")

par_points_nb <- 5000
par_case_ratio <- 1/20
par_size_gamma_shape <- 0.95
par_size_gamma_scale  <- 10
```

A set of points is drawn by a Poisson process (whose expectation of the number of points is `r format(as.numeric(par_points_nb), scientific=FALSE, big.mark=",")`) in a square window of side 1.
Each point is assigned a qualitative mark: 'Case' or 'Control'.
`r (1 - par_case_ratio)* 100`% of points are 'Controls'.
`r par_case_ratio * 100`% are 'Cases', whose spatial structure is studied.
The weight of the points is drawn from a gamma distribution with free shape and scale parameters.

```{r}
#| label: XcsrCode
X_csr <- function(
    points_nb,
    case_ratio = par_case_ratio,
    size_gamma_shape = par_size_gamma_shape,
    size_gamma_scale = par_size_gamma_scale) {
  points_nb %>% 
    runifpoint() %>% 
    as.wmppp() ->
    X
  cases_nb <- round(points_nb *  case_ratio)
  controls_nb <- points_nb - cases_nb
  c(rep("Control", controls_nb), rep("Case", cases_nb)) %>% 
    as.factor() ->
    X$marks$PointType
  rgamma(
    X$n, 
    shape = size_gamma_shape, 
    scale = size_gamma_scale
  ) %>% 
    ceiling() ->
    X$marks$PointWeight
  X
}

# Example
X <- X_csr(par_points_nb)
# Map the cases
autoplot(X[X$marks$PointType == "Case"])
```

In this example, the drawing of points is completely random (*complete spatial randomness*: CSR), i.e. there is no simulation of attraction or dispersion of points which could generate spatial concentrations of points (aggregates) or, on the contrary, spatial regularities (dispersions).
Sets of aggregated points can be drawn in a @Matern1960 process.

```{r}
#| label: ParamsMaternCode
# Expected number of clusters
par_kappa <- 20
# Cluster radius
par_scale <-  0.1
```

(ref:XMatern) Random draw of a set of points where the Cases (in red) are aggregated and the Controls (in blue) are distributed completely randomly. The size of the points is proportional to their weight.

```{r}
#| label: XMaternFig
#| include: true
#| fig.cap: "(ref:XMatern)"
X_matern <- function(
    points_nb,
    case_ratio = par_case_ratio,
    kappa = par_kappa,
    scale = par_scale,
    size_gamma_shape = par_size_gamma_shape,
    size_gamma_scale = par_size_gamma_scale) {
  cases_nb <- round(points_nb *  case_ratio)
  controls_nb <- points_nb - cases_nb
  # CSR controls
  controls_nb %>% 
    runifpoint() %>% 
    superimpose(
      # Matern cases
      rMatClust(
        kappa = kappa, 
        scale = scale, 
        mu = cases_nb / kappa
      ) 
    ) %>% 
    as.wmppp() ->
    X
  # Update the number of cases
  cases_nb <- X$n - controls_nb
  c(rep("Control", controls_nb), rep("Case", cases_nb)) %>% 
    as.factor() ->
    X$marks$PointType
  rgamma(
    X$n, 
    shape = size_gamma_shape, 
    scale = size_gamma_scale
  ) %>% 
    ceiling() ->
    X$marks$PointWeight
  X
}

# Example
X <- X_matern(par_points_nb)
# Map the cases
autoplot(X) + 
  scale_size(range = c(0, 3))
```

The Cases are shown in figure \@ref(fig:XMaternFig): the aggregates are clearly visible.
The controls are distributed completely randomly.

## Gridding the space

Let's consider the simulation of the Cases obtained by the Matérn process and cut the window into a grid.
It simulates the usual approximation of the position of the points of an administrative unit to the position of its centre.

```{r}
#| label: ParamsPartitionsCode
# Number of rows and columns
par_partitions <- 20
```

```{r}
#| label: group_pointsCode
# Group points into cells
group_points <- function(X, partitions = par_partitions) {
  X %>%
    with(tibble(
      x, 
      y, 
      PointType = marks$PointType, 
      PointWeight = marks$PointWeight)
    ) %>% 
    mutate(
      x_cell = ceiling(x * partitions) / partitions - 1 / 2 / partitions,
      y_cell = ceiling(y * partitions) / partitions - 1 / 2 / partitions,
      .keep = "unused"
    ) %>%
    rename(x = x_cell, y = y_cell) %>% 
    as.wmppp(window = X$window, unitname = X$window$units) %>% 
    rjitter()
}
# Group points and merge them
group_points_to_plot <- function(X, partitions = par_partitions) {
  X %>%
    with(tibble(
      x, 
      y, 
      PointType = marks$PointType, 
      PointWeight = marks$PointWeight)
    ) %>% 
    mutate(
      x_cell = ceiling(x * partitions) / partitions - 1 / 2 / partitions,
      y_cell = ceiling(y * partitions) / partitions - 1 / 2 / partitions
    ) %>%
    group_by(PointType, x_cell, y_cell) %>% 
    summarise(n = n(), PointWeight = sum(PointWeight)) %>% 
    rename(x = x_cell, y = y_cell) %>% 
    as.wmppp(window = X$window, unitname = X$window$units)
}
```

The approximated position of points is shown on the map in figure \@ref(fig:GroupedFig).
Each cell now contains only one point of each type, whose weight is the sum of the weights of the individual points.

(ref:Grouped) Repositioning of points in an arbitrary grid. The absence of Cases in a cell is easily detected (single-colour blue dot), as is the strong presence of Cases in a cell (two-colour dot, but predominantly red).

```{r}
#| label: GroupedFig
#| include: true
#| fig.cap: "(ref:Grouped)"
X %>% group_points_to_plot() %>% autoplot(alpha = 0.5)
```

The values of the *M* function can now be calculated from the original point set or its approximation after recentring.

# Computing *M* with the *dbmss* package

```{r}
#| label: ParamsrCode
r <- c((0:9) / 100, (2:10) / 20)
```

## Necessary data

In the *dbmss* package, the function is applied to a set of points or a distance matrix.
The set of points in figure \@ref(fig:XMaternFig) is used.
The distance matrix between all the pairs of its points is calculated to form the data on which the performance tests will be carried out.

```{r}
#| label: DtableMCode
# Extract a dataframe from the point set
points_df <- with(X, data.frame(x, y, marks))
head(points_df)
```

## Point pattern

The `Mhat()` function in the *dbmss* package is used to estimate the *M* function.
The theoretical reference value for *M* is 1, as this function relates the proportion of Cases up to a distance $r$ to that observed over the entire window.
The aggregation of Cases will be highlighted by values of *M* greater than 1 (the relative presence of Cases is greater locally than over the whole window) and the dispersion of Cases by values less than 1.
We observe (figure \@ref(fig:MFig)) that *M* detects an agglomeration of Cases, which is in line with the simulation of this type of point (the controls having a completely random location on the window).
The advantage of a function based on distances is clearly visible: it allows us to detect exactly at which distance(s) the attraction phenomena occur and are the most important (for functions whose values can be compared at different radii, such as *M*).
In addition to estimating the *M* function, the `Menvelope()` function can be used to calculate its global confidence interval [@Duranton2005] under the null hypothesis of random point location.
It allows parallezing the necessary simulations.
The result is shown in figure \@ref(fig:MFig).

(ref:M) Value of *M* as a function of distance from the reference point. The confidence interval, simulated at 95%, appears in grey and is centred on the value 1.

```{r}
#| label: MFig
#| include: true
#| fig.cap: "(ref:M)"
X %>% 
  MEnvelope(r = r, ReferenceType = "Case", Global = TRUE) %>% 
  autoplot() +
  theme(
    legend.position = c(0.7, 0.75), 
    legend.background = element_blank()
  )
```

## Distance matrix

```{r}
#| label: DtableCode
d_matrix <- as.Dtable(points_df)
```

Matrices can be used to process non-Euclidean distances (transport time, road distance, etc.) which cannot be represented by a set of points.
The `Mhat()` and `MEnvelope()` functions are the same, and provide the same results whatever the form of the data used here (point set or distance matrix).

# Computational performance

The use of the *M* function to characterise the spatial structure of large sets of points may be limited by the computing time or memory required.

```{r}
#| label: XtoMCode
# Compute M
X_to_M <- function(X) {
  X %>% 
    Mhat(r = r, ReferenceType = "Case") %>% 
    pull("M")    
}
```

## Computing time

Calculating the distances between all pairs of points is necessary to estimate *M*.
The calculation time is therefore expected to increase as the square of the number of points.
The calculation time required for the exact calculation is evaluated for a range of numbers of points (figure \@ref(fig:TestTimeMemFig)a).

```{r}
#| label: ParamsXSizeCode
X_sizes <- c(1000, 5000, 10000, 50000, 100000)
```

```{r}
#| label: TestTime
library("microbenchmark")
test_time <- function(points_nb) {
  X <- X_csr(points_nb)
  microbenchmark(X_to_M(X), times = 4L) %>% 
    pull("time")
}

X_sizes %>% 
  sapply(FUN = test_time) %>% 
  as_tibble() %>% 
  pivot_longer(cols = everything()) %>% 
  rename(Size = name) %>% 
  group_by(Size) %>% 
  summarise(Time = mean(value) / 1E9, sd = sd(value) / 1E9) %>% 
  mutate(
    Size = as.double(
      plyr::mapvalues(
        .$Size, 
        from = paste0("V", seq_along(X_sizes)), 
        to = X_sizes
      )
    )
  ) -> M_time
```

```{r}
#| label: TimeModelCode
# Model
M_time %>% 
  mutate(logTime = log(Time), logSize = log(Size)) ->
  M_time_log
M_time_lm <- lm(logTime ~ logSize, data = M_time_log) 
summary(M_time_lm)
```

**The calculation time is related to the size of the set of points by a power law.** It increases less quickly than the square of the number of points.
It can be estimated very precisely ($R^2=$ `r format(summary(M_time_lm)$r.squared, digits=2)`) by the relation $t=t_0 (n/n_o)^p$ where $t$ is the estimated time for $n$ points (e.g.: `r format(as.numeric(M_time[5, 2]), digits=3)` seconds for `r format(as.numeric(M_time[5, 1]), scientific=FALSE, big.mark=",")` points) knowing the time $t_0$ for $n_0$ points and $p$ is the power relation (here: `r format(M_time_lm$coefficients[2], digits=2)`).

```{r}
#| label: MmbCode
library("microbenchmark")
microbenchmark(
  Mhat(X, r = r, ReferenceType = "Case", NeighborType = "Control"),
  Mhat(d_matrix, r = r, ReferenceType = "Case", NeighborType = "Control"),
  times = 4L
) %>% 
  summary() %>% 
  as.data.frame() %>%
  select(expr, median) %>% 
  mutate(
    `Method` = c("Point Pattern", "Distance Matrix"), 
    `Time` = round(median),
    .before = expr,
    .keep = "none" 
  ) -> Mmb
```

Using a distance matrix may seem an efficient way of saving computation time, but in reality calculating distances is extremely fast and the whole process from a matrix is ultimately more time-consuming.
The median execution time is equal to `r Mmb$Time[1]` milliseconds for estimating the *M* function from a set of `r format(as.numeric(par_points_nb), scientific=FALSE, big.mark=",")` points or `r Mmb$Time[2]` milliseconds for the corresponding distance matrix.

## Memory

```{r}
#| label: TestMem
# RAM
library("profmem")
test_ram <- function(points_nb) {
  X <- X_csr(points_nb)
  profmem(X_to_M(X)) %>% 
    pull("bytes") %>% 
    sum(na.rm = TRUE)
}
sapply(X_sizes, FUN = test_ram) %>% 
  tibble(Size = X_sizes, RAM = . / 2^20) ->
  M_ram
```

(ref:TestMemTime) Calculation time (a) in seconds and memory required (b) in MB to estimate the *M* function as a function of the size of the set of points. The bars represent the $\pm 1$ standard deviation interval.

```{r}
#| label: TestTimeMemFig
#| include: true
#| fig.asp: 0.5
#| fig.cap: "(ref:TestMemTime)"
#| fig.env: "figure*"
#| out.extra: ""

library("gridExtra")
M_time %>% 
  ggplot(aes(x = Size, y = Time)) +
    geom_point() +
    geom_errorbar(aes(ymin = Time - sd, ymax = Time + sd)) +
    scale_x_log10() +
    scale_y_log10() + 
    labs(tag = "a") ->
  TestTimeMemFiga
M_ram %>% 
  ggplot(aes(x = Size, y = RAM)) +
    geom_point() +
    geom_line() + 
    labs(tag = "b") +
    theme(axis.text.x = element_text(size = 10)) ->
  TestTimeMemFigb
grid.arrange(TestTimeMemFiga, TestTimeMemFigb, ncol = 2)
```

The memory used is evaluated for the same data sizes (figure \@ref(fig:TestTimeMemFig)b).
**The memory required increases linearly with the number of points and is never critical for point set sizes that can be processed in reasonable times.** This highlights @Tidu2023's conclusion about the power and computation time required when using *M* on large datasets.
The memory used by `Dtable` objects to calculate $M$ from a distance matrix is much greater: it is that of a numerical matrix, of the order of 8 bytes times the number of points squared, i.e. 800 MB for 10,000 points only.
As the calculation time is not reduced by this approach, its use should be reserved for non-Euclidean distances.

# Effects of approximating the position of points

```{r}
#| label: ParamsSimsCode
simulations_n <- 100
```

Clearly, approximating the position of the points results in a loss of information: in each grid cell, the distance between all the points is set to zero, and the distance between two points in different cells is approximated by the distance between the centroids of the two cells.
We therefore expect a severe error in the estimation of *M* on a small scale (of the order of magnitude of the size of the cells) and an error that decreases with distance, when the relative size of the cells decreases.
The effect of the location approximation is first tested on a set of aggregated points, similar to the real @Tidu2023 data.
Secondly, the case of an unstructured set of points is considered.

## Case of an aggregated distribution (Matérn)

```{r}
#| label: XMaternList
# Simulate X
X_matern_list <- replicate(
  simulations_n, 
  expr = X_matern(par_points_nb), 
  simplify = FALSE
)
# Group points and compute M
X_matern_grouped_list <- lapply(
  X_matern_list, 
  FUN = group_points, 
  partitions = par_partitions
)
```

`r simulations_n` sets of aggregated points (`r format(as.numeric(par_points_nb), scientific=FALSE, big.mark=",")` points with `r par_case_ratio * 100`% of Cases) are simulated.
To evaluate the effect of the position approximation, the exact calculation and the calculation on the grid points are performed on each set of points.

```{r}
#| label: MMaternTimeCode
library("pbapply")
# Compute M
M_matern_original <- pbsapply(X_matern_list, FUN = X_to_M)
M_matern_grouped <- pbsapply(X_matern_grouped_list, FUN = X_to_M)
```

(ref:MapproxMatern) Average estimate of the *M* function from the exact position of the points compared with the values obtained by grouping the points (a) and correlation between them (b). Cases form aggregates of radius `r format(par_scale, digits=1)`.

```{r}
#| label: MapproxMaternFig
#| include: true
#| fig.asp: 0.5
#| fig.cap: "(ref:MapproxMatern)"
#| fig.env: "figure*"
#| out.extra: ""

tibble(
  r,
  Exact = rowMeans(M_matern_original), 
  Grouped =  rowMeans(M_matern_grouped)
) %>% 
  pivot_longer(
    cols = !r,
    names_to = "M",
    values_to = "value"
  ) %>% 
  ggplot(aes(x = r, y = value, color = M)) +
  geom_line() +
  geom_point() +
  labs(x = "Distance", y = "M", tag = "a") +
    theme(
    legend.position = c(0.75, 0.8),
    legend.background = element_blank()
  ) -> 
MapproxMaternFiga
# Correlation
M_cor <- function(r_value, M_original, M_grouped) {
  r_index <- which(r == r_value)
  # Return
  c(
    # Distance
    r_value,
    # Correlation
    cor(M_original[r_index, ], M_grouped[r_index, ])
  ) 
}
sapply(
  r, 
  FUN = M_cor, 
  M_original = M_matern_original, 
  M_grouped = M_matern_grouped
) %>%
  t() %>% 
  as_tibble() %>% 
  rename(r = V1, correlation = V2) %>% 
  ggplot(aes(x = r, y = correlation)) +
    geom_point() +
    geom_line() +
    labs(x = "Distance", y = "Correlation", tag = "b") -> 
  MapproxMaternFigb
grid.arrange(MapproxMaternFiga, MapproxMaternFigb, ncol = 2)
```

The mean values of the estimates of *M* are presented in figure \@ref(fig:MapproxMaternFig)a.
The size of the grid cells is equal to `r 1/par_partitions`.
All neighbours at distances less than this threshold are placed at zero distance: the estimate of the function is constant up to this threshold and small-scale aggregation is underestimated.
The correlation between the *M* values estimated by each method is calculated at each distance (figure \@ref(fig:MapproxMaternFig)b).

**The correlation is very close to 1, and the estimated values very similar, as soon as the distance taken into account exceeds the grid cell: the approximation is not a problem if the interactions between the points are studied beyond this distance.** The information on interactions at short distances, i.e. within each grid cell, is lost, or, more precisely, approximated by its value at the grid scale.

## Case of a completely random distribution (CSR)

```{r}
#| label: XCSRListCode
# Simulate X
X_csr_list <- replicate(
  simulations_n, 
  expr = X_csr(par_points_nb), 
  simplify = FALSE
)
# Group points and compute M
X_csr_grouped_list <- lapply(
  X_csr_list, 
  FUN = group_points, 
  partitions = par_partitions
)
```

The same simulations are run with a completely random set of points.
The exact calculation and the calculation on the grid points are carried out on each set of points.

```{r}
#| label: MCSRTimeCode
# Compute M
system.time(M_csr_original <- pbsapply(X_csr_list, FUN = X_to_M))
system.time(M_csr_grouped <- sapply(X_csr_grouped_list, FUN = X_to_M))
```

(ref:MapproxCSR) Average estimate of the *M* function from the exact position of the points compared with the values obtained by grouping the points (a) and correlation between them (b). Both Cases and Controls are drawn in a Poisson process.

```{r}
#| label: MapproxCSRFig
#| include: true
#| fig.asp: 0.5
#| fig.cap: "(ref:MapproxCSR)" 
#| fig.env: "figure*"
#| out.extra: ""

tibble(
  r,
  Exact = rowMeans(M_csr_original), 
  Grouped =  rowMeans(M_csr_grouped)
) %>% 
  pivot_longer(
    cols = !r,
    names_to = "M", 
    values_to = "value"
  ) %>% 
  ggplot(aes(x = r, y = value, color = M)) +
  geom_line() +
  geom_point() +
  labs(x = "Distance", y = "M", tag = "a") +
  theme(
    legend.position = c(0.75, 0.2),
    legend.background = element_blank()
  ) ->
MapproxCSRFiga
# Correlation
sapply(
  r, 
  FUN = M_cor, 
  M_original = M_csr_original, 
  M_grouped = M_csr_grouped
) %>%
  t() %>% 
  as_tibble() %>% 
  rename(r = V1, correlation = V2) %>% 
  ggplot(aes(x = r, y = correlation)) +
    geom_point() +
    geom_line() +
    labs(x = "Distance", y = "Correlation", tag = "b") ->
  MapproxCSRFigb
grid.arrange(MapproxCSRFiga, MapproxCSRFigb, ncol = 2)
```

The average values are shown in figure \@ref(fig:MapproxCSRFig)a.
The mean value of *M* is equal to 1 at all distances by construction: Cases and Controls are distributed completely randomly.
The approximations are relatively small in value (a few percent) but artefactual aggregation is generated at small scale.
As the real value of *M* varies little around 1, the correlations are much weaker (figure \@ref(fig:MapproxCSRFig)b) in the absence of spatial structure than in the aggregated case.

# Discussion and conclusion

Our results are in line with @Tidu2023's article, which mentions strong correlations between *M* values computed from exact and approximated Italian company location data.
Since the spatial structure of their data is probably an intermediate case between the two cases dealt with in our article (aggregated and random theoretical distributions), the results provided by our two contributions are complementary.

The computation burden of estimating the *M* function on large datasets may be an issue.
The calculation time for *M* is around 5 seconds for a set of 100,000 points on a laptop (Intel i7-1360P 2.20 GHz processor), and requires 25 MB of RAM.
Calculating a confidence interval from 1,000 simulations therefore takes less than two hours.
For a set of five million points, the power law predicts around an hour of computing time.
1,000 simulations would then take more than one month.
Thanks to parallelization, a calculation server would drastically increase performance, but at the cost of a complexity of implementation that limits its use.
If we limit ourselves to the computing power of a personal computer, **exact calculation is fully justified for data of the order of $10^5$ points**: a few hours are enough to calculate confidence intervals.
Since parallelizing the simulations is offered with no effort by the *dbmss* package, this time can be reduced by a factor depending on the available hardware, say 2 to 6 with modern multicore CPU's.

Beyond that, approximating the location reduces the size of the set of points to the number of locations selected.
Again, it may be up to $10^5$ locations to keep computing time acceptable, whatever the size of the original dataset.
The choice of the size of the grid [or the administrative scale of the aggregation of points in @Tidu2023] must be done according to the scale of the interactions under study: they can not be characterized correctly at distances below it.

So, **approximation on location can be considered to save computation time, given the strong correlation observed between the values of *M* on exact and approximated data, but the scale of the grid must be fine enough to be informational at small distances**.
Choosing the approximation scale is a tradeoff between accuracy, i.e. a small distance threshold above which results are accurate, and speed with a coarse grid.


# Appendix {.unnumbered}

R code is available at the following address: <https://ericmarcon.github.io/MLargeDataSets/Appendix.pdf>

# Acknowledgements {.unnumbered}

Eric Marcon benefited from an 'Investissement d’Avenir' grant managed by the Agence Nationale de la Recherche (LABEX CEBA, ref. ANR-10-LBX-25) and Florence Puech gratefully acknowledges financial support from INRAE.

`r if (!knitr::is_latex_output()) '# References {-}'`
